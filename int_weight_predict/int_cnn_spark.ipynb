{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试写一个整形权重CNN的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb 1\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1337)  # for reproducibility\n",
    "nb_classes = 34\n",
    "nb_epoch = 30\n",
    "batch_size = 128\n",
    "\n",
    "image_higth, image_width = 15, 15\n",
    "# lr = [0.05, 0.01, 0.005]\n",
    "layer1 = 10\n",
    "hidden1 = 40\n",
    "region = 3\n",
    "\n",
    "# 模型权重根目录\n",
    "model_root_path = '/home/jdwang/PycharmProjects/digitRecognition/int_weight_predict/modelAndData1122/'\n",
    "model_file_list_path = os.listdir(os.path.join(model_root_path, 'model'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义必要的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_valdata(version='1122'):\n",
    "    \"\"\"读取不同版本的测试集\n",
    "    1120 - 对应 /home/jdwang/PycharmProjects/digitRecognition/int_weight_predict/modelAndData1120/Data/\n",
    "    1122 - 对应 /home/jdwang/PycharmProjects/digitRecognition/int_weight_predict/modelAndData1122/Data/\n",
    "\n",
    "    :param dir_path: str\n",
    "    :param version: str\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # 数据集根目录\n",
    "    data_root_path = '/home/jdwang/PycharmProjects/digitRecognition/int_weight_predict/'\n",
    "\n",
    "    # 读取验证数据、测试数据\n",
    "    if version == '1122':\n",
    "        val_file_path = os.path.join(data_root_path, 'modelAndData1122/Data/', 'TrainSet_trainAndVal_testSet.pickle')\n",
    "        other_file_path = os.path.join(data_root_path, 'modelAndData1122/Data/', 'Olddata_TestSet.pickle')\n",
    "        with open(val_file_path, 'rb') as train_file:\n",
    "            train_X = pickle.load(train_file)\n",
    "            train_y = pickle.load(train_file)\n",
    "            val_X, val_y = None, None\n",
    "            test_X = pickle.load(train_file)\n",
    "            test_y = pickle.load(train_file)\n",
    "            \n",
    "            \n",
    "    elif version == '1120':\n",
    "        val_file_path = os.path.join(data_root_path, 'modelAndData1120/Data/', 'valSet&TestSet.pickle')\n",
    "        other_file_path = os.path.join(data_root_path, 'modelAndData1120/Data/', 'Olddata_TestSet.pickle')\n",
    "        with open(val_file_path, 'rb') as train_file:\n",
    "            val_X = pickle.load(train_file)\n",
    "            val_y = pickle.load(train_file)\n",
    "            test_X = pickle.load(train_file)\n",
    "            test_y = pickle.load(train_file)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    with open(other_file_path, 'rb') as otherFile:\n",
    "        other_X = pickle.load(otherFile)\n",
    "        other_y = pickle.load(otherFile)\n",
    "\n",
    "    return (val_X, np.asarray(val_y)), (test_X, np.asarray(test_y)), (other_X, np.asarray(other_y))\n",
    "\n",
    "\n",
    "def Net_model(layer1, hidden1, region, rows, cols, nb_classes, lr=0.01, decay=1e-6, momentum=0.9):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "    from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "    from keras.optimizers import SGD\n",
    "    from keras import backend as K\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution2D(layer1, region, region,\n",
    "                            border_mode='valid',\n",
    "                            input_shape=(1, rows, cols)))\n",
    "\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())  # 平铺\n",
    "\n",
    "    model.add(Dense(hidden1))  # Full connection 1:  1000\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    # 获取CNN的中间结果\n",
    "    mid_output = K.function(inputs=[\n",
    "        model.layers[0].input,\n",
    "        K.learning_phase(),\n",
    "    ],\n",
    "        outputs=[\n",
    "            model.layers[-9].output,\n",
    "            model.layers[-8].output,\n",
    "            model.layers[-7].output,\n",
    "            model.layers[-6].output,\n",
    "            model.layers[-5].output,\n",
    "            model.layers[-4].output,\n",
    "            model.layers[-3].output,\n",
    "            model.layers[-2].output,\n",
    "            model.layers[-1].output,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    sgd = SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "    return model, mid_output\n",
    "\n",
    "\n",
    "def test_model(model_file, X_test, Y_test):\n",
    "    # 加载模型架构\n",
    "    # 这里的 lr 设置什么不影响\n",
    "    model, mid_output = Net_model(layer1, hidden1, region, image_higth, image_width, nb_classes=34, lr=0)\n",
    "    model.load_weights(model_file)\n",
    "\n",
    "    # 预测\n",
    "    predicted = model.predict_classes(X_test, verbose=0)\n",
    "\n",
    "    return count_result(predicted, Y_test)\n",
    "\n",
    "\n",
    "def count_result(predicted, Y_test):\n",
    "    '''统计预测情况，包括混淆集个数等\n",
    "\n",
    "    :return:\n",
    "    '''\n",
    "    # 统计混淆集\n",
    "    badcase = {}\n",
    "    for i in range(0, len(Y_test)):\n",
    "        if (tramsform(predicted[i]) in ['1', 'I'] and tramsform(Y_test[i]) in ['1', 'I']):\n",
    "            # 1跟I区分，只要 是 测试 成1或I，而实际值是 1或I都算对\n",
    "            predicted[i] = Y_test[i]\n",
    "        if predicted[i] != Y_test[i]:\n",
    "            ch1 = tramsform(Y_test[i])\n",
    "            ch2 = tramsform(predicted[i])\n",
    "            string = ','.join(sorted([ch1, ch2]))\n",
    "\n",
    "            if badcase.has_key(string):\n",
    "                badcase[string] += 1\n",
    "            else:\n",
    "                badcase[string] = 1\n",
    "\n",
    "    # 计算测试准确率\n",
    "    test_accuracy = np.mean(np.equal(predicted, Y_test))\n",
    "    graterThan2 = 0\n",
    "    graterThan5 = 0\n",
    "    graterThan10 = 0\n",
    "    for key, value in badcase.items():\n",
    "        if value >= 2:\n",
    "            graterThan2 += 1\n",
    "        if value >= 5:\n",
    "            graterThan5 += 1\n",
    "        if value >= 10:\n",
    "            graterThan10 += 1\n",
    "\n",
    "    return test_accuracy, graterThan2, graterThan5, graterThan10, badcase\n",
    "\n",
    "\n",
    "def tramsform(num):\n",
    "    \"\"\" 数字 转换成 字符\n",
    "\n",
    "    :param num:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if num < 10:\n",
    "        return str(num)\n",
    "    else:\n",
    "        if num >= 24:\n",
    "            num += 1\n",
    "        if num >= 31:\n",
    "            num += 1\n",
    "        return chr(ord('A') + num - 10)\n",
    "\n",
    "\n",
    "def conv_pool_operation(img, conv_W, conv_b):\n",
    "    \"\"\"CNN 卷积 和 池化操作\n",
    "        一张图片\n",
    "    :param img: array-3D\n",
    "        一张图片，3D，(num_of_channels,img_height,img_width)\n",
    "    :param conv_W: array-4D\n",
    "        10*3*3\n",
    "    :param conv_b:\n",
    "    :return: array-3D\n",
    "    \"\"\"\n",
    "    filter_row, filter_col = conv_W.shape[2:]\n",
    "    img_row, img_col = img.shape[1:]\n",
    "    # convolution\n",
    "    # 3D\n",
    "    conv_result = np.zeros((conv_W.shape[0], img_row - filter_row + 1, img_col - filter_col + 1))\n",
    "    # quit()\n",
    "    for filter_index in range(conv_W.shape[0]):\n",
    "        j = conv_W[filter_index, 0].flatten()[-1::-1]\n",
    "        for x in range(0, img_row - filter_row + 1):\n",
    "            for y in range(0, img_col - filter_col + 1):\n",
    "#                 i = img[0, x:x + filter_row, y:y + filter_col].flatten()\n",
    "#                 \n",
    "                conv_result[filter_index,\n",
    "                            x,\n",
    "                            y] = tanh_approximate_function(\n",
    "                    np.dot(\n",
    "                        img[0, x:x + filter_row, y:y + filter_col].flatten(), \n",
    "                        j\n",
    "                    ) \n",
    "                    + conv_b[filter_index]\n",
    "                )\n",
    "                # print(np.dot(i, j) + conv_b[filter_index])\n",
    "\n",
    "    # conv_result = tanh_approximate_function(conv_result)\n",
    "    pool_row, pool_col = 2, 2\n",
    "    pool_result = np.zeros(\n",
    "        (conv_result.shape[0], conv_result.shape[1] / pool_row, conv_result.shape[2] / pool_col))\n",
    "    # max-pooling\n",
    "    for channel_index in range(pool_result.shape[0]):\n",
    "        for x in range(pool_result.shape[1]):\n",
    "            for y in range(pool_result.shape[2]):\n",
    "                pool_result[\n",
    "                    channel_index,\n",
    "                    x,\n",
    "                    y\n",
    "                ] = np.max(\n",
    "                    conv_result[\n",
    "                    channel_index,\n",
    "                    x * pool_row: (x + 1) * pool_row,\n",
    "                    y * pool_col: (y + 1) * pool_col\n",
    "                    ])\n",
    "    return pool_result\n",
    "\n",
    "\n",
    "def hidden_operation(feature_vector, W, b, activion='tanh'):\n",
    "    \"\"\" 隐含层操作\n",
    "\n",
    "    :param activion: str\n",
    "        激活函数\n",
    "    :param feature_vector: array-like\n",
    "        一张 图片的 feature vector，1D\n",
    "    :param W: array-2D\n",
    "        权重\n",
    "    :param b: array-1D\n",
    "        偏差\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if activion == 'tanh':\n",
    "        # return np.asarray([tanh_approximate_function(item) for item in temp])\n",
    "        return map(lambda x: tanh_approximate_function(np.dot(feature_vector, x[0]) + x[1]), zip(W.transpose(), b))\n",
    "    elif activion == 'none':\n",
    "        # return temp\n",
    "        return map(lambda x:np.dot(feature_vector, x[0])+x[1], zip(W.transpose(),b))\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def tanh_approximate_function(x):\n",
    "    \"\"\"\n",
    "         * tanh x=sinh x / cosh x\n",
    "         * 其中sinh x=(e^(x)-e^(-x))/2 ，cosh x=(e^x+e^(-x))/2\n",
    "         * 所以tanhx = (e^(x)-e^(-x)) /(e^x+e^(-x))\n",
    "    \"\"\"\n",
    "    #     return (int)(tanh(x));\n",
    "\n",
    "    if x > 10:\n",
    "        return 1\n",
    "    elif x < -10:\n",
    "        return -1\n",
    "    else:\n",
    "        return (int)((np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x)))\n",
    "\n",
    "\n",
    "def cnn_batch_predict(X_val, weights):\n",
    "    '''CNN批量预测\n",
    "\n",
    "    :param X_val:\n",
    "    :param weights:\n",
    "    :return:\n",
    "    '''\n",
    "    weights = [(item * 1e6).astype(dtype=int) for item in weights]\n",
    "    # result = cnn_predict(X_val[0],weights)\n",
    "    result = []\n",
    "    for index, img in enumerate(X_val):\n",
    "        if (index + 1) % 1000 == 0:\n",
    "            print('%d' % (index + 1))\n",
    "        result.append(cnn_predict(img, weights))\n",
    "    return np.asarray(result)\n",
    "\n",
    "\n",
    "def cnn_predict(img, weights):\n",
    "    '''单张图片的预测\n",
    "\n",
    "    :param img:\n",
    "    :param weights:\n",
    "    :return:\n",
    "    '''\n",
    "    # 3D\n",
    "    %time imgs_conv_result = conv_pool_operation(img, weights[0], weights[1])\n",
    "    # print('conv over..')\n",
    "    %time flatten_result = imgs_conv_result.flatten()\n",
    "    # print('flatten over..')\n",
    "    # print(flatten_result.shape)\n",
    "    %time hidden1_output = hidden_operation(flatten_result, weights[2], weights[3], activion='tanh')\n",
    "\n",
    "    # print('hidden1 over..')\n",
    "    %time hidden2_output = hidden_operation(hidden1_output, weights[4], weights[5], activion='none')\n",
    "    # print('hidden2 over..')\n",
    "    # print(hidden2_output.shape)\n",
    "    result = np.argmax(hidden2_output)\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def cnn_batch_predict_spark(X_val_rdd,y_val, weights):\n",
    "    weights = [(item * 1e5).astype(dtype=int) for item in weights]\n",
    "#     result_rdd = X_val_rdd.map(lambda x:cnn_predict(x,weights))\n",
    "    \n",
    "    result_rdd = X_val_rdd.map(lambda x:conv_pool_operation(x, weights[0], weights[1]))\\\n",
    "    .map(lambda x: x.flatten())\\\n",
    "    .map(lambda x:hidden_operation(x,weights[2], weights[3],activion='tanh'))\\\n",
    "    .map(lambda x:hidden_operation(x,weights[4], weights[5],activion='none'))\\\n",
    "    .map(np.argmax)\n",
    "    \n",
    "# #     result_rdd.cache()\n",
    "    result = result_rdd.collect()\n",
    "#     accu = np.mean(np.asarray(result) == y_val) \n",
    "# #     print(accu)\n",
    "    return np.asarray(result),count_result(result,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64381, 1, 15, 15)\n",
      "(243391, 1, 15, 15)\n",
      "CPU times: user 4.12 s, sys: 232 ms, total: 4.35 s\n",
      "Wall time: 4.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# region 读取数据集：验证数据(64369个)、测试数据(64381个)、其他应用数据集(243391个)\n",
    "(X_val, y_val), (X_test, y_test), (X_other, y_other) = load_valdata(version='1122')\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_other.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将图片数据序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 ms, sys: 56 ms, total: 112 ms\n",
      "Wall time: 1.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 测试集\n",
    "X_val_rdd = sc.parallelize(X_test)\n",
    "y_val = y_test\n",
    "X_val_rdd.cache\n",
    "\n",
    "# 应用集\n",
    "X_other_rdd = sc.parallelize(X_other)\n",
    "y_other = y_other\n",
    "\n",
    "X_other_rdd.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 保存34分类TOP21的测试集预测结果"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "%%time\n",
    "\n",
    "# 34分类前21个模型的预测结果\n",
    "predict_result_34class_file = open(os.path.join(model_root_path,'34class_test_predict_result_e5.pkl'),'w')\n",
    "\n",
    "# TOP21个模型的编号\n",
    "run_id = [99,129,141,245,249,270,287,300,311,375,425,509,543,630,758,864,875,890,905,975,1014]\n",
    "run_idx = 0\n",
    "with open(os.path.join(model_root_path, '34class_model_weight.pkl'), 'r') as fin:\n",
    "    for index in range(1, len(model_file_list_path) + 1):\n",
    "        # 从 模型1 开始，依次往后\n",
    "        # 找到对应模型文件\n",
    "#         if index != run_id[run_idx]:\n",
    "#             weights = pickle.load(fin)\n",
    "#             continue\n",
    "        \n",
    "        if index < run_id[run_idx]:\n",
    "            weights = pickle.load(fin)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        weights = pickle.load(fin)\n",
    "\n",
    "        # print('OK')\n",
    "        int_predict,(test_accuracy, graterThan2, graterThan5, graterThan10,badcase) = \\\n",
    "        cnn_batch_predict_spark(X_val_rdd,y_val, weights)\n",
    "        \n",
    "        print(index,test_accuracy, graterThan2, graterThan5,graterThan10)\n",
    "        print(badcase)\n",
    "        \n",
    "        pickle.dump(np.asarray(int_predict),\n",
    "                    predict_result_34class_file\n",
    "                   )\n",
    "\n",
    "        \n",
    "#         print(test_accuracy, graterThan5, graterThan10)\n",
    "        run_idx += 1\n",
    "        if run_idx>=len(run_id):\n",
    "            break\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 整理下badcase混淆集情况"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "s='''{'3,8': 2, 'X,Y': 1, '0,Q': 2, '7,P': 1, '1,7': 3, '1,T': 3, 'C,G': 1, '7,T': 1, '8,B': 1, '3,S': 2, '4,A': 1, '0,D': 1, '0,C': 1, '5,L': 2, '2,Z': 2, 'G,Q': 1, 'H,K': 1}\n",
    "{'9,S': 1, '0,Q': 1, 'H,W': 1, 'F,P': 1, '1,T': 2, '8,B': 4, '6,G': 4, '5,U': 2, 'D,Q': 1, '0,D': 5, 'A,X': 1, '2,Z': 2, 'C,Q': 1}\n",
    "{'A,N': 1, '6,K': 1, '0,Q': 2, 'F,P': 1, 'C,G': 1, 'E,F': 2, '6,8': 2, '0,D': 2, '2,Z': 2}\n",
    "{'1,X': 1, '0,Q': 5, '1,7': 3, '1,4': 3, 'C,G': 1, '6,E': 1, '0,G': 2, '6,8': 2, '0,D': 3, '0,C': 1, '8,Y': 1, 'F,P': 2, '5,B': 1, '5,S': 1, '1,T': 2, '3,B': 2}\n",
    "{'9,S': 1, '5,S': 1, '8,B': 1, '5,U': 2, '0,G': 2, '4,A': 4, '0,D': 3, '0,C': 2, '2,Z': 3, '6,F': 1, 'C,Q': 1, '3,9': 1}\n",
    "{'7,Z': 1, '3,8': 2, '0,Q': 2, '2,E': 2, '1,7': 5, '2,Z': 2, '8,B': 1, '6,F': 1, 'F,H': 1, '0,D': 6, '5,L': 2, '5,S': 1, 'G,Q': 1}\n",
    "{'0,Q': 6, 'F,P': 1, '1,T': 3, '8,B': 2, '5,U': 2, '0,G': 2, '6,8': 1, '0,D': 2, '0,C': 2, 'B,R': 1, 'K,X': 1}\n",
    "{'F,P': 3, '5,S': 2, '1,T': 1, 'C,G': 1, '8,B': 1, '5,U': 2, 'F,J': 2, 'G,U': 1, '0,D': 5, '0,C': 3, '2,Z': 2, '5,B': 1, '1,G': 2, 'M,Y': 4, '2,P': 5}\n",
    "{'C,G': 2, '3,8': 2, '0,Q': 3, '6,B': 1, '8,B': 1, '0,G': 2, '0,D': 6, '5,N': 2, 'X,Y': 1, '2,Z': 1, 'B,D': 1}\n",
    "{'P,R': 1, '0,Q': 1, '1,7': 1, '1,T': 3, '7,T': 1, '8,B': 4, '6,8': 3, '0,D': 3, '9,B': 1, '5,L': 2, '2,Z': 2, '8,S': 2}\n",
    "{'9,S': 1, '0,Q': 4, '5,S': 1, '1,T': 2, 'C,G': 5, '8,B': 3, 'F,P': 1, '4,A': 1, '0,D': 7, '1,J': 1, '2,Z': 3, 'N,W': 2}\n",
    "{'3,8': 2, '9,S': 1, '1,T': 5, '3,7': 1, '8,B': 3, '0,9': 1, '6,8': 2, '0,D': 1, '2,Z': 4, '3,B': 1}\n",
    "{'3,8': 2, '3,9': 1, '0,Q': 2, '5,S': 1, '6,E': 1, '5,U': 2, 'D,Q': 1, '0,D': 7, 'F,P': 2, '5,B': 2, '6,S': 1, '8,U': 1}\n",
    "{'J,T': 1, '7,Z': 1, '0,Q': 1, '1,T': 2, '7,T': 4, '8,B': 1, '0,9': 1, '0,D': 4, '2,Z': 1, 'G,Q': 1, '6,S': 1, 'C,Q': 2}\n",
    "{'1,7': 3, '1,T': 1, '8,B': 2, '5,U': 2, '3,M': 2, '0,D': 3, '0,C': 3, '8,Y': 1, '5,S': 1, '8,S': 1, '3,B': 4}\n",
    "{'A,N': 1, 'X,Y': 1, '1,7': 2, '1,T': 3, '8,B': 1, 'F,P': 1, '6,8': 1, '4,A': 1, '0,D': 2, '0,C': 1, '5,L': 2, '2,Z': 1, '5,B': 1, 'A,W': 1}\n",
    "{'J,T': 2, '3,8': 3, '0,Q': 1, '0,1': 3, 'F,P': 4, '1,T': 2, 'M,N': 5, '8,B': 2, '6,8': 1, '0,D': 4, '2,Z': 1, '6,S': 1, '7,T': 1, 'M,X': 1}\n",
    "{'9,S': 1, '0,Q': 1, '1,7': 2, 'D,Q': 1, '5,T': 1, '5,U': 2, '0,G': 3, '4,A': 4, '0,D': 2, '0,C': 1, '2,Z': 1, '8,U': 1, '6,F': 1, '7,F': 2, 'D,P': 1}\n",
    "{'7,Y': 1, '0,Q': 4, 'A,I': 1, '1,T': 2, '8,B': 1, 'D,Q': 1, '0,D': 1, '9,B': 1, '2,Z': 1}\n",
    "{'3,8': 2, '5,S': 1, '1,T': 1, 'C,G': 1, '7,T': 3, '8,B': 1, 'F,P': 1, '6,8': 1, '4,A': 2, '0,D': 5, '2,Z': 2, 'G,Q': 1}\n",
    "{'3,8': 2, '9,S': 2, '0,Q': 1, '5,S': 1, '0,D': 5, '2,Z': 4, '5,B': 1, '6,S': 1, '2,R': 3}'''\n",
    "print(s)\n",
    "\n",
    "# 收集 混淆集 集合\n",
    "key_set = []\n",
    "for item in s.split('\\n'):\n",
    "    a = eval(item)\n",
    "    key_set += a.keys()\n",
    "\n",
    "key_set = sorted(set(key_set))\n",
    "print(key_set)\n",
    "\n",
    "result = [] \n",
    "for item in s.split('\\n'):\n",
    "    a = eval(item)\n",
    "    for i in key_set:\n",
    "        if i not in a.keys():\n",
    "            a[i]=0 \n",
    "#     print(a)\n",
    "    \n",
    "    print([item[1] for item in sorted(a.items(),key = lambda x:x[0])])\n",
    "    \n",
    "    result.append(sorted(a.items(),key = lambda x:x[0]))\n",
    "#     break\n",
    "\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 保存34分类TOP21的应用集预测结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 34分类前21个模型的预测结果\n",
    "predict_result_34class_file = open(os.path.join(model_root_path,'34class_other22_predict_result_e5.pkl'),'w')\n",
    "run_id = [99,129,141,245,249,270,287,300,311,375,425,509,543,630,758,864,875,890,905,975,1014]\n",
    "# run_id = [287,300,311,375,425,509,543,630,758,864]\n",
    "# run_id = [875,890,905,975,1014]99,129,141,245,249,270,\n",
    "run_idx = 0\n",
    "with open(os.path.join(model_root_path, '34class_model_weight.pkl'), 'r') as fin:\n",
    "    for index in range(1, len(model_file_list_path) + 1):\n",
    "        # 从 模型1 开始，依次往后\n",
    "        # 找到对应模型文件\n",
    "#         if index != run_id[run_idx]:\n",
    "#             weights = pickle.load(fin)\n",
    "#             continue\n",
    "        \n",
    "        if index < run_id[run_idx]:\n",
    "            weights = pickle.load(fin)\n",
    "            continue\n",
    "\n",
    "        \n",
    "        weights = pickle.load(fin)\n",
    "\n",
    "        # print('OK')\n",
    "        int_predict,(test_accuracy, graterThan2, graterThan5, graterThan10,badcase) = \\\n",
    "                                            cnn_batch_predict_spark(X_other_rdd,y_other, weights)\n",
    "        \n",
    "        print(index,test_accuracy, graterThan2, graterThan5,graterThan10)\n",
    "        print(badcase)\n",
    "        \n",
    "        pickle.dump(np.asarray(int_predict),\n",
    "                    predict_result_34class_file\n",
    "                   )\n",
    "\n",
    "        \n",
    "#         print(test_accuracy, graterThan5, graterThan10)\n",
    "        run_idx += 1\n",
    "        if run_idx>=len(run_id):\n",
    "            break\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存34分类TOP21的 测试集/应用集 预测结果（用5-6二分类器修正）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "# option = 'test'\n",
    "option = 'other'\n",
    "\n",
    "if option == 'test':\n",
    "    predict_result_34class_file = open(os.path.join(model_root_path, '34class_test_predict_result_e5.pkl'), 'r')\n",
    "    X = X_test\n",
    "    y = y_test\n",
    "elif option == 'other':\n",
    "    predict_result_34class_file = open(os.path.join(model_root_path, '34class_other1_predict_result_e5.pkl'), 'r')\n",
    "    X = X_other\n",
    "    y = y_other\n",
    "\n",
    "run_id = [ 287, 300, 311, 375, 425, 509, 543, 630, 758, 864]\n",
    "# run_id = [99, 129, 141, 245, 249, 270, 287, 300, 311, 375, 425, 509, 543, 630, 758, 864, 875, 890, 905, 975, 1014]\n",
    "# run_id = [129, 141, 245, 249, 270, 287, 300, 311, 375, 425, 509, 543, 630, 758, 864, 875, 890, 905, 975, 1014]\n",
    "# 指定修正某个34分类结果\n",
    "start = 864\n",
    "for index in run_id:\n",
    "    int_predict_34class = np.asarray(pickle.load(predict_result_34class_file))\n",
    "    if index != start:\n",
    "        continue\n",
    "    print(index)\n",
    "    # 读取二分类器的权重\n",
    "    weights = pickle.load(open(os.path.join(model_root_path, '56binary_model_weight.pkl'), 'r'))\n",
    "    # 被预测成56的数据\n",
    "    idx_predicted_56 = (int_predict_34class == 5) + (int_predict_34class == 6)\n",
    "    print('预测成5-6的个数:%d' % sum(idx_predicted_56))\n",
    "\n",
    "\n",
    "    X_rdd = sc.parallelize(X[idx_predicted_56])\n",
    "    X_rdd.cache()\n",
    "\n",
    "    binary_result,(test_accuracy, graterThan2, graterThan5, graterThan10,badcase)= cnn_batch_predict_spark(\n",
    "        X_rdd, \n",
    "        y[idx_predicted_56],\n",
    "        weights\n",
    "    )\n",
    "    \n",
    "    binary_result[binary_result == 0] = 5\n",
    "    binary_result[binary_result == 1] = 6\n",
    "#     print(binary_result)\n",
    "    # 修正结果\n",
    "    int_predict_34class[idx_predicted_56] = binary_result\n",
    "    print(index, count_result(int_predict_34class, y))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "run_id = [99, 129, 141, 245, 249, 270, 287, 300, 311, 375, 425, 509, 543, 630, 758, 864, 875, 890, 905, 975, 1014]\n",
    "# fileout = open(os.path.join(model_root_path,'34class_other111_predict_result_e5.pkl'),'w')\n",
    "predict_result_34class_file = open(os.path.join(model_root_path,'34class_other_predict_result_e5.pkl'),'r')\n",
    "tt1 = []\n",
    "for i in run_id:\n",
    "    print(i)\n",
    "    temp = pickle.load(predict_result_34class_file)\n",
    "    tt1.append(temp)\n",
    "#     print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
